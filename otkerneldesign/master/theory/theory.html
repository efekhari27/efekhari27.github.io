
<!DOCTYPE html>

<html lang="python">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

    <title>Principle of kernel-based sampling methods &#8212; otkerneldesign 0.1.4 documentation</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/openturns.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css" />
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/sphinx_highlight.js"></script>
    <link rel="shortcut icon" href="../_static/Icon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Index of classes" href="../user_manual/user_manual.html" />
    <link rel="prev" title="Documentation" href="../index.html" />
    <link href='http://fonts.googleapis.com/css?family=Open+Sans:300,400,700'
          rel='stylesheet' type='text/css' />
 

  </head><body>
<div class="pageheader">
  <ul>
    <li><a href="../index.html">Home</a></li>
    <li><a href="../user_manual/user_manual.html">Doc</a></li>
    <li><a href="../examples/examples.html">Examples</a></li>
  </ul>
  <a href="../index.html">
    <h1>
      <img src="../_static/logo-openturns-wo-bg.png" alt="" width=100px height=100px />
      otkerneldesign
    </h1>
    <h2> Module otkerneldesign </h2>
  </a>
</div>

    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../user_manual/user_manual.html" title="Index of classes"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="../index.html" title="Documentation"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">otkerneldesign 0.1.4 documentation</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">Principle of kernel-based sampling methods</a></li> 
      </ul>
    </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <div>
    <h3><a href="../index.html">Table of Contents</a></h3>
    <ul>
<li><a class="reference internal" href="#">Principle of kernel-based sampling methods</a><ul>
<li><a class="reference internal" href="#kernel-herding">Kernel herding</a></li>
<li><a class="reference internal" href="#greedy-support-points">Greedy support points</a></li>
<li><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
</ul>

  </div>
  <div>
    <h4>Previous topic</h4>
    <p class="topless"><a href="../index.html"
                          title="previous chapter">Documentation</a></p>
  </div>
  <div>
    <h4>Next topic</h4>
    <p class="topless"><a href="../user_manual/user_manual.html"
                          title="next chapter">Index of classes</a></p>
  </div>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../_sources/theory/theory.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>
        </div>
      </div>

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <section id="principle-of-kernel-based-sampling-methods">
<h1>Principle of kernel-based sampling methods<a class="headerlink" href="#principle-of-kernel-based-sampling-methods" title="Permalink to this heading">¶</a></h1>
<p>This section presents kernel-based sampling methods developped in <code class="docutils literal notranslate"><span class="pre">otkerneldesign</span></code> for design of experiments, numerical integration and quantization.</p>
<p><strong>Introduction.</strong></p>
<p>Let us denote by <img class="math" src="../_images/math/e9a40c93b7361068df31c3cb5cf68f30658c7d71.svg" alt="\vect{X}_n = \left\{\vect{x}^{(1)},\ldots, \vect{x}^{(n)}\right\} \in \cD_\vect{X} \subset \Rset^p" style="vertical-align: -6px"/>
the <img class="math" src="../_images/math/f084bb373e531b0883dc3788190588413b9cabf5.svg" alt="n" style="vertical-align: 0px"/>-sample of input realizations, also called input &quot;design of experiments&quot; (DoE) or simply design.</p>
<p>From a computer experiment point of view, let us consider a costly function <img class="math" src="../_images/math/554ac591605372f7d9dabfd3e84634fc5d1eef9a.svg" alt="g:\cD_\vect{X} \rightarrow \Rset" style="vertical-align: -3px"/>,
a first goal of the following DoE methods is to explore the input space in a space-filling way (e.g., to build a generic machine learning model of <img class="math" src="../_images/math/bce642d6b4d9ec929df5cf046fabf620641553a4.svg" alt="g" style="vertical-align: -3px"/>).
However, this work will exploit these methods with a specific purpose in mind: to numerically integrate <cite>g</cite>
against the probability density function <img class="math" src="../_images/math/645bf66da37a8e5678879fcd6bbd0d26f9d5a9bd.svg" alt="f_{\vect{X}}" style="vertical-align: -3px"/> which relates to a central tendency estimation of an output
random variable <img class="math" src="../_images/math/21d7cb93ad6e448c4b7b0df266dd43d01c35b818.svg" alt="Y=g(\vect{X})" style="vertical-align: -4px"/>, resulting from an uncertainty propagation step.</p>
<p>Sampling methods based on the notion of discrepancy between distributions in a kernel-based
functional space were used to approximate integrals. More precisely, one can mention the use
of the distance called the <em>maximum mean discrepancy</em> (MMD) as a core ingredient of advanced sampling
methods such as the <em>Support points</em> by (Mak &amp; Joseph, 2018) and the <em>Kernel herding</em> by (Chen &amp; al., 2010).
Manipulating the MMD is convenient since its expression is simple ; it depends on an arbitrarily chosen kernel.
Let us setup the introduction of the Kernel herding and Support points methods by briefly defining a few mathematical concepts.</p>
<p><strong>Reproducing kernel Hilbert space.</strong></p>
<p>Assuming that <img class="math" src="../_images/math/772911ebc10cabc6c4701689a854f82d15c57836.svg" alt="k" style="vertical-align: 0px"/> is a symmetric and positive definite function <img class="math" src="../_images/math/5f69a6c8cd61f9cdcf5b53062ecd87881c53ed63.svg" alt="k: \cD_\vect{X} \times \cD_\vect{X} \rightarrow \Rset" style="vertical-align: -2px"/>,
latter called a &quot;reproducing kernel&quot; or simply a &quot;kernel&quot;. A <em>reproducing kernel Hilbert space</em> (RKHS) is an inner product
space <img class="math" src="../_images/math/3ad92572a83a47bda350f98359fb17b2668da29b.svg" alt="\cH(k)" style="vertical-align: -4px"/> of functions <img class="math" src="../_images/math/554ac591605372f7d9dabfd3e84634fc5d1eef9a.svg" alt="g:\cD_\vect{X} \rightarrow \Rset" style="vertical-align: -3px"/> with the following properties:</p>
<ul class="simple">
<li><p><img class="math" src="../_images/math/caa910a9bc097d4a1ef43626d745115364c0a270.svg" alt="k(\cdot, \vect{x}) \in \cH(k), \quad \forall \vect{x} \in \cD_\vect{X}." style="vertical-align: -4px"/></p></li>
<li><p><img class="math" src="../_images/math/5c9cd0282851eae4e4ac555bec655de72d5e4988.svg" alt="\langle g, k(\cdot, \vect{x}) \rangle_{\cH(k)} = g(\vect{x}), \quad \forall \vect{x} \in \cD_\vect{X}, \forall g \in \cH(k)" style="vertical-align: -5px"/></p></li>
</ul>
<p>Note that for a defined reproducing kernel, a unique RKHS exists and vice versa (see <a class="reference external" href="https://arxiv.org/pdf/2109.06075.pdf">C.Oates, 2021</a> ).</p>
<p><strong>Potential.</strong></p>
<p>For any target distribution <img class="math" src="../_images/math/a5e12eead60991d1c277108c221717bd28e18e50.svg" alt="\mu" style="vertical-align: -3px"/>, its <em>potential</em> (also called &quot;kernel mean embedding&quot;) associated with the kernel <img class="math" src="../_images/math/772911ebc10cabc6c4701689a854f82d15c57836.svg" alt="k" style="vertical-align: 0px"/> is defined as:</p>
<div class="math" id="potential">
<span id="equation-potential"></span><p><span class="eqno">(1)<a class="headerlink" href="#potential" title="Permalink to this equation">¶</a></span><img src="../_images/math/7beed5ff7bcc8a0ec79691f5b0da90ac18f99e65.svg" alt="P_{\mu}(\vect{x}) := \int_{\cD_\vect{X}} k(\vect{x}, \vect{x}') \di \mu(\vect{x}')."/></p>
</div><p>Then, the potential of a discrete distribution <img class="math" src="../_images/math/18e0564a66b640f62828b22dbd02b6e7ed892ff3.svg" alt="\zeta_n = \frac1n \sum_{i=1}^{n} \delta(\vect{x}^{(i)})" style="vertical-align: -6px"/>
(uniform mixture of Dirac distributions at the design points <img class="math" src="../_images/math/93868c612d0f4e38543b65fde12ceec728a3dbbc.svg" alt="\vect{X}_n" style="vertical-align: -2px"/>) associated with the kernel <img class="math" src="../_images/math/772911ebc10cabc6c4701689a854f82d15c57836.svg" alt="k" style="vertical-align: 0px"/> can be expressed as:</p>
<div class="math" id="design-potential">
<span id="equation-design-potential"></span><p><span class="eqno">(2)<a class="headerlink" href="#design-potential" title="Permalink to this equation">¶</a></span><img src="../_images/math/96b4e675200acbce383cb88d4b98fe26b77856b0.svg" alt="P_{\zeta_n}(\vect{x}) = \frac1n \sum_{i=1}^{n} k(\vect{x}, \vect{x}^{(i)})."/></p>
</div><p>Close potentials can be interpreted to mean that the design <img class="math" src="../_images/math/93868c612d0f4e38543b65fde12ceec728a3dbbc.svg" alt="\vect{X}_n" style="vertical-align: -2px"/> adequately quantizes <img class="math" src="../_images/math/a5e12eead60991d1c277108c221717bd28e18e50.svg" alt="\mu" style="vertical-align: -3px"/></p>
<p><strong>Maximum mean discrepancy.</strong></p>
<p>A metric of discrepancy and quadrature error is offered by the MMD.
This distance between two distributions <img class="math" src="../_images/math/a5e12eead60991d1c277108c221717bd28e18e50.svg" alt="\mu" style="vertical-align: -3px"/> and <img class="math" src="../_images/math/bd702322c33f4e506bf550cd7d653701c0083ebb.svg" alt="\zeta" style="vertical-align: -3px"/> is given by the
maximal quadrature error committed for any function within the unit ball of an RKHS:</p>
<div class="math" id="mmd">
<span id="equation-mmd"></span><p><span class="eqno">(3)<a class="headerlink" href="#mmd" title="Permalink to this equation">¶</a></span><img src="../_images/math/6e489f4a41d58a53d07f4f4957a4ed52ac0a80f8.svg" alt="\mathrm{MMD}_k(\mu, \zeta) :=
\sup_{\lVert g \lVert_{\cH(k)} \leq 1}
        \left | \int_{\cD_{\vect{X}}} g(\vect{x}) \di \mu(\vect{x}) - \int_{\cD_{\vect{X}}} g(\vect{x}) \di \zeta(\vect{x}) \right|."/></p>
</div><p>Using the Cauchy-Schwartz inequality, one can demonstrate that
<img class="math" src="../_images/math/0993599709c291c8da3eafeb4abd240024ee912a.svg" alt="\mathrm{MMD}_k(\mu, \zeta) = \left\lVert P_{\mu}(\vect{x}) - P_{\zeta}(\vect{x}) \right\lVert_{\cH(k)}" style="vertical-align: -8px"/>
(see <a class="reference external" href="https://arxiv.org/pdf/2109.06075.pdf">C.Oates, 2021</a> ).
Moreover, a kernel <img class="math" src="../_images/math/772911ebc10cabc6c4701689a854f82d15c57836.svg" alt="k" style="vertical-align: 0px"/> is called characteristic if <img class="math" src="../_images/math/9274f00f9a0cf2141dfff067d7b10890524f78e2.svg" alt="\mathrm{MMD}_k(\mu, \zeta) = 0" style="vertical-align: -4px"/> is equivalent to <img class="math" src="../_images/math/6e66060a13b25c4eac469266312269414569eaf7.svg" alt="\mu = \zeta" style="vertical-align: -3px"/>.</p>
<section id="kernel-herding">
<h2>Kernel herding<a class="headerlink" href="#kernel-herding" title="Permalink to this heading">¶</a></h2>
<p>In this section we introduce the Kernel herding (KH) (Chen &amp; al., 2010), a sampling method which intends to
minimize a squared MMD by adding points iteratively. Considering a design <img class="math" src="../_images/math/93868c612d0f4e38543b65fde12ceec728a3dbbc.svg" alt="\vect{X}_n" style="vertical-align: -2px"/> and its corresponding
discrete distribution <img class="math" src="../_images/math/a12e984ad718338b7999d7a6c8483a24625ccaa8.svg" alt="\zeta_n= \frac{1}{n} \sum_{i=1}^{n} \delta(\vect{x}^{(i)})" style="vertical-align: -6px"/>, a KH iteration can be written as
an optimization over the point <img class="math" src="../_images/math/64c7eb7ae192f2b65e7f634ecbd37d78661a99a2.svg" alt="\vect{x}^{(n+1)} \in \cD_{\vect{X}}" style="vertical-align: -2px"/> of the following criterion:</p>
<div class="math" id="kh-criterion">
<span id="equation-kh-criterion"></span><p><span class="eqno">(4)<a class="headerlink" href="#kh-criterion" title="Permalink to this equation">¶</a></span><img src="../_images/math/4d2b93f2953131c67a511c5eee71e20fc8cb7dbd.svg" alt="\vect{x}^{(n+1)} \in \argmin_{\vect{x} \in \mathcal{S}} \left(P_{\zeta_n}(\vect{x}) - P_{\mu}(\vect{x})\right)"/></p>
</div><p>considering a kernel <img class="math" src="../_images/math/772911ebc10cabc6c4701689a854f82d15c57836.svg" alt="k" style="vertical-align: 0px"/> and a given set <img class="math" src="../_images/math/4ed7549180e74e8b53c335d6b88467391f2c62f2.svg" alt="\cS\subseteq\cD_{\vect{X}}" style="vertical-align: -2px"/> of candidate points
(e.g., a fairly dense finite subset with size <img class="math" src="../_images/math/766088526b7a09f15c544e41f5cd4a611783646b.svg" alt="N \gg n" style="vertical-align: -1px"/> that emulates the target distribution).
This compact criterion derives from the expression of a descent algorithm with respect to <img class="math" src="../_images/math/aeb45085564e42e15778abda19e3eca7f13d613a.svg" alt="\vect{x}_{n+1}" style="vertical-align: -4px"/>
(see (Pronzato &amp; Zhigljavsky, 2020) for the full proof).</p>
<p>In practice, <img class="math" src="../_images/math/103ee4744f10ce99412a947f9b02dda40ef6ef4e.svg" alt="P_{\mu}(\vect{x})" style="vertical-align: -5px"/> can be expressed analytically in the specific cases of input distribution and kernel
(e.g., for independent uniform or normal inputs and a Matérn <img class="math" src="../_images/math/d3cd7fe547696875ec4f1b452a375c4bd4bcb469.svg" alt="5/2" style="vertical-align: -4px"/> kernel (Fekhari &amp; al., 2021)), making the computation very fast.
Alternatively, the potential can be evaluated on an empirical measure <img class="math" src="../_images/math/b356f3748f9a614157a0f2c1aac1f6169fff2789.svg" alt="\mu_N" style="vertical-align: -3px"/>, substituting <img class="math" src="../_images/math/a5e12eead60991d1c277108c221717bd28e18e50.svg" alt="\mu" style="vertical-align: -3px"/>,
formed by a dense and large-size sample of <img class="math" src="../_images/math/a5e12eead60991d1c277108c221717bd28e18e50.svg" alt="\mu" style="vertical-align: -3px"/> (e.g., the candidate set <img class="math" src="../_images/math/c233d76c0183d8ef5c1bbc3362e122ba1b1e738f.svg" alt="\mathcal{S}" style="vertical-align: 0px"/>).
<img class="math" src="../_images/math/103ee4744f10ce99412a947f9b02dda40ef6ef4e.svg" alt="P_{\mu}(\vect{x})" style="vertical-align: -5px"/> is then approached by <img class="math" src="../_images/math/e0d9315bd8b0fc7054b2c3a0091483ebd4b63de7.svg" alt="P_{\mu_N}(\vect{x}) = (1/N)\, \sum_{j=1}^N k(\vect{x}, \vect{x}'^{(j)})" style="vertical-align: -7px"/>,
which can be injected in <a class="reference internal" href="#kh-criterion"><span class="std std-ref">(4)</span></a> to solve the following optimization:</p>
<div class="math" id="kh-estimation">
<span id="equation-kh-estimation"></span><p><span class="eqno">(5)<a class="headerlink" href="#kh-estimation" title="Permalink to this equation">¶</a></span><img src="../_images/math/9f45daa369aad85357a09ba4c8b3286b11edc9c6.svg" alt="\vect{x}^{(n+1)} \in \argmin_{\vect{x}\in\mathcal{S}} \left( \frac{1}{n} \sum_{i=1}^{n} k(\vect{x},\vect{x}^{(i)})
  - \frac{1}{N} \sum_{j=1}^N k(\vect{x},\vect{x}'^{(j)}) \right) \,."/></p>
</div><p>When no observation is available, which is the common situation at the design stage,
the kernel hyperparameters (e.g., correlation lengths) have to be set to heuristic values.
MMD minimization is quite versatile and was explored in details by (Teymur &amp; al., 2021)
or (Pronzato &amp; Zhigljavsky, 2020), however the method is very sensitive to the kernel chosen and its tuning.
Support points is a closely related method with a more rigid mathematical structure but interesting performances.</p>
</section>
<section id="greedy-support-points">
<h2>Greedy support points<a class="headerlink" href="#greedy-support-points" title="Permalink to this heading">¶</a></h2>
<p>Support points (SP) (Mak &amp; Joseph, 2018) are such that their associated empirical distribution
<img class="math" src="../_images/math/bace780f1ee9e6783b6b8ee937aa5864188f1769.svg" alt="\zeta_n" style="vertical-align: -3px"/> has minimum energy distance with respect to a target distribution <img class="math" src="../_images/math/a5e12eead60991d1c277108c221717bd28e18e50.svg" alt="\mu" style="vertical-align: -3px"/>.
This criterion can be seen as a particular case of the MMD for the characteristic &quot;energy-distance&quot; kernel given by:</p>
<div class="math" id="energy-kernel">
<span id="equation-energy-kernel"></span><p><span class="eqno">(6)<a class="headerlink" href="#energy-kernel" title="Permalink to this equation">¶</a></span><img src="../_images/math/df4c9de0561315399dafc89343f0099f52a43d2b.svg" alt="k_E(\vect{x},\vect{x}') = \frac{1}{2}\, \left(\| \vect{x} \| + \| \vect{x}' \| - \| \vect{x}-\vect{x}' \|\right)\,."/></p>
</div><p>Compared to more heuristic methods for solving quantization problems, Support points
benefit from the theoretical guarantees of MMD minimization in terms of convergence of <img class="math" src="../_images/math/bace780f1ee9e6783b6b8ee937aa5864188f1769.svg" alt="\zeta_n" style="vertical-align: -3px"/> to <img class="math" src="../_images/math/a5e12eead60991d1c277108c221717bd28e18e50.svg" alt="\mu" style="vertical-align: -3px"/> as <img class="math" src="../_images/math/596facc6f54c4f9fd1b67d325ae5e31a055f0cb4.svg" alt="n\to\infty" style="vertical-align: 0px"/>.</p>
<p>At first sight, this optimization problem seems intractable, although (Mak &amp; Joseph, 2018) propose to
rewrite the function as a difference of convex functions in <img class="math" src="../_images/math/93868c612d0f4e38543b65fde12ceec728a3dbbc.svg" alt="\vect{X}_n" style="vertical-align: -2px"/>, which yields a difference-of-convex program.
To simplify the algorithm and keep an iterative design, a different approach will be used here.
At iteration <img class="math" src="../_images/math/e04a83b75a1895000081f86388223e57b75c8315.svg" alt="n+1" style="vertical-align: -1px"/>, the algorithm solves greedily the MMD minimization between <img class="math" src="../_images/math/bace780f1ee9e6783b6b8ee937aa5864188f1769.svg" alt="\zeta_n" style="vertical-align: -3px"/> and <img class="math" src="../_images/math/a5e12eead60991d1c277108c221717bd28e18e50.svg" alt="\mu" style="vertical-align: -3px"/> for the candidate set <img class="math" src="../_images/math/c233d76c0183d8ef5c1bbc3362e122ba1b1e738f.svg" alt="\mathcal{S}" style="vertical-align: 0px"/>:</p>
<div class="math" id="greedy-criterion">
<span id="equation-greedy-criterion"></span><p><span class="eqno">(7)<a class="headerlink" href="#greedy-criterion" title="Permalink to this equation">¶</a></span><img src="../_images/math/e25c1e7d7a9324a056555f8d047721066130364e.svg" alt="\vect{x}^{(n+1)} \in \argmin_{\vect{x}\in\mathcal{S}} \Bigg( \frac{1}{N} \sum_{j=1}^N \|\vect{x}-\vect{x}'^{(j)}\|
- \frac{1}{n+1} \sum_{i=1}^{n} \|\vect{x}-\vect{x}^{(i)}\| \Bigg) \,."/></p>
</div><p>For this criterion, one can notice that it is almost identical to the KH one in <a class="reference internal" href="#kh-criterion"><span class="std std-ref">(4)</span></a> when
taking as kernel the energy-distance kernel given in <a class="reference internal" href="#energy-kernel"><span class="std std-ref">(6)</span></a>.
These two iterative methods were exploited in (Fekhari &amp; al., 2021) to study new ways to construct
a validation set for machine learning models by conveniently selecting a test set for a better model performance estimation.</p>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this heading">¶</a></h2>
<ul class="simple">
<li><p>Chen, Y., M. Welling, &amp; A. Smola (2010). Super-samples from kernel herding. In Proceedings of the Twenty-Sixth
Conference on Uncertainty in Artificial Intelligence, pp. 109 – 116. <a class="reference external" href="https://arxiv.org/pdf/1203.3472.pdf">PDF</a></p></li>
<li><p>Mak, S. &amp; V. R. Joseph (2018). Support points. The Annals of Statistics 46, 2562 – 2592. <a class="reference external" href="https://projecteuclid.org/journals/annals-of-statistics/volume-46/issue-6A/Support-points/10.1214/17-AOS1629.full">PDF</a></p></li>
<li><p>Fekhari, E., B. Iooss, J. Mure, L. Pronzato, &amp; M. Rendas (2022). Model predictivity assessment: incremental
test-set selection and accuracy evaluation. preprint. <a class="reference external" href="https://hal.archives-ouvertes.fr/hal-03523695/document">PDF</a></p></li>
<li><p>Briol, F.-X., C. Oates, M. Girolami, M. Osborne, &amp; D. Sejdinovic (2019). Probabilistic Integration: A Role in
Statistical Computation? Statistical Science 34, 1 – 22. <a class="reference external" href="https://projecteuclid.org/journals/statistical-science/volume-34/issue-1/Rejoinder-Probabilistic-Integration-A-Role-in-Statistical-Computation/10.1214/18-STS683.full">PDF</a></p></li>
<li><p>Pronzato, L. &amp; A. Zhigljavsky (2020). Bayesian quadrature and energy minimization for space-filling design.
SIAM/ASA Journal on Uncertainty Quantification 8, 959 – 1011 <a class="reference external" href="https://hal.archives-ouvertes.fr/hal-01864076v3/document">PDF</a></p></li>
<li><p>Huszár, F. &amp; D. Duvenaud (2012). Optimally-Weighted Herding is Bayesian Quadrature. In Proceedings of the
Twenty-Eighth Conference on Uncertainty in Artificial Intelligence, pp. 377 – 386. <a class="reference external" href="https://arxiv.org/pdf/1204.1664.pdf">PDF</a></p></li>
<li><p>Teymur, O., J. Gorham, M. Riabiz, &amp; C. Oates (2021). Optimal quantisation of probability measures using
maximum mean discrepancy. In International Conference on Artificial Intelligence and Statistics, pp. 1027 – 1035. <a class="reference external" href="http://proceedings.mlr.press/v130/teymur21a/teymur21a.pdf">PDF</a></p></li>
</ul>
</section>
</section>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../user_manual/user_manual.html" title="Index of classes"
             >next</a> |</li>
        <li class="right" >
          <a href="../index.html" title="Documentation"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">otkerneldesign 0.1.4 documentation</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">Principle of kernel-based sampling methods</a></li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2022, OpenTURNS.
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 5.2.3.
    </div>
  </body>
</html>